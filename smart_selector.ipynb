{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\warun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\warun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Code Changes   \n",
      "0    if x > 5:\\n    result = x * 2\\nelse:\\n    resu...  \\\n",
      "1    from database import connect_db\\n\\ndef get_use...   \n",
      "2    def divide(a, b):\\n    return a / b\\n\\n\\ndef t...   \n",
      "3    class Calculator:\\n    def add(self, a, b):\\n ...   \n",
      "4    def process_data(data):\\n    if data:\\n       ...   \n",
      "..                                                 ...   \n",
      "200  class Stack:\\n    def __init__(self):\\n       ...   \n",
      "201  def analyze_data(data, analysis_type='basic'):...   \n",
      "202  def calculate_area(base, height, shape='triang...   \n",
      "203  class EmailSender:\\n    def __init__(self, smt...   \n",
      "204              numerator = 5\\nresult = numerator / 0   \n",
      "\n",
      "    Smart Test Selection Mechanism  \n",
      "0           Changed Code Selection  \n",
      "1       Dependency-Based Selection  \n",
      "2     Historical Failure Selection  \n",
      "3        Coverage-Driven Selection  \n",
      "4        Impact Analysis Selection  \n",
      "..                             ...  \n",
      "200     Dependency-Based Selection  \n",
      "201   Historical Failure Selection  \n",
      "202      Impact Analysis Selection  \n",
      "203       Priority Queue Selection  \n",
      "204      Coverage-Driven Selection  \n",
      "\n",
      "[205 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Using pytorh \n",
    "#tensorflow way\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('csv.csv')\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17, Loss: 1.7637227773666382\n",
      "Epoch 2/17, Loss: 1.7418612241744995\n",
      "Epoch 3/17, Loss: 1.7014516592025757\n",
      "Epoch 4/17, Loss: 1.6553884744644165\n",
      "Epoch 5/17, Loss: 1.4415475130081177\n",
      "Epoch 6/17, Loss: 1.3485851287841797\n",
      "Epoch 7/17, Loss: 1.2781450748443604\n",
      "Epoch 8/17, Loss: 1.254122257232666\n",
      "Epoch 9/17, Loss: 1.2256451845169067\n",
      "Epoch 10/17, Loss: 1.1572619676589966\n",
      "Epoch 11/17, Loss: 1.1806937456130981\n",
      "Epoch 12/17, Loss: 1.2686513662338257\n",
      "Epoch 13/17, Loss: 1.2339955568313599\n",
      "Epoch 14/17, Loss: 1.276078701019287\n",
      "Epoch 15/17, Loss: 1.14839768409729\n",
      "Epoch 16/17, Loss: 1.179030179977417\n",
      "Epoch 17/17, Loss: 1.0586559772491455\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming your data is in a pandas DataFrame called 'df1'\n",
    "code_changes = df1['Code Changes'].tolist()\n",
    "smart_test_selection = df1['Smart Test Selection Mechanism'].tolist()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(code_changes)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(code_changes)\n",
    "\n",
    "# Padding sequences for consistent input size\n",
    "input_sequences = pad_sequences(input_sequences)\n",
    "\n",
    "# Convert 'smart_test_selection' to labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(smart_test_selection)\n",
    "\n",
    "# Define PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create PyTorch Dataset and DataLoader\n",
    "dataset = CustomDataset(input_sequences, encoded_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define PyTorch Model\n",
    "class PyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, output_size):\n",
    "        super(PyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate PyTorch Model\n",
    "pymodel = PyModel(vocab_size=total_words, embedding_dim=100, lstm_hidden_dim=100, output_size=len(set(encoded_labels)))\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pymodel.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 17\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pymodel(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PyTorch model\n",
    "torch.save(pymodel.state_dict(), 'pymodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyModel(\n",
       "  (embedding): Embedding(392, 100)\n",
       "  (lstm): LSTM(100, 100, batch_first=True)\n",
       "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the PyTorch model\n",
    "loaded_pymodel = PyModel(vocab_size=total_words, embedding_dim=100, lstm_hidden_dim=100, output_size=len(set(encoded_labels)))\n",
    "loaded_pymodel.load_state_dict(torch.load('pymodel.pth'))\n",
    "loaded_pymodel.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted Smart Test Selection Mechanism is: Priority Queue Selection\n"
     ]
    }
   ],
   "source": [
    "#STILL BEST\n",
    "# Assuming 'new_code_snippet' is the new Python code snippet you want to predict on\n",
    "new_code_snippet = \"\"\"class EmailSender:\\n    def __init__(self, smtp_server, smtp_port, username, password):\\n        self.smtp_server = smtp_server\\n        self.smtp_port = smtp_port\\n        self.username = username\\n        self.password = password\\n\\n    def send_email(self, recipient, subject, message):\\n        # Code to send an email\\n        pass\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Example code snippet\n",
    "\n",
    "# Tokenize and pad the new code snippet\n",
    "new_code_sequence = tokenizer.texts_to_sequences([new_code_snippet])\n",
    "padded_new_code_sequence = pad_sequences(new_code_sequence)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "input_tensor = torch.LongTensor(padded_new_code_sequence)\n",
    "\n",
    "# Make predictions using the loaded PyTorch model\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_pymodel(input_tensor)\n",
    "    _, predicted_class_index = torch.max(outputs, 1)\n",
    "\n",
    "# Convert the predicted class index back to the original label\n",
    "predicted_label = label_encoder.inverse_transform([predicted_class_index.item()])\n",
    "\n",
    "print(f\"The predicted Smart Test Selection Mechanism is: {predicted_label[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
